# chap10 동적비전

## 10.1 모션 분석
물체의 모션 정보를 분석하는 알고리즘     
</br>   
비디오 video : 시간 순서에 따라 정지 영상을 나열한 구조
- 동영상 dynamic image 라고도 부름
- 프레임 frame : 비디오를 구성하는 영상 한 장, 2차원 공간에 정의
- 프레임 + 시간 축을 3차원 시공간 형성
- 컬러 영상의 프레임 m x n x 3 구조 텐서, T장의 프레임을 담은 비디오는 m x n x 3 x T의 4차원 구조 텐

![image](https://user-images.githubusercontent.com/109460178/230826555-a15d9048-c47f-4a54-b076-2f9a408fafec.png)

딥러닝 이전엔, 배경이 고정된 상황에서는 **"차영상"** 을 분석해 정보 획득
- t 순간 프레임의 (j,i) 화솟값
- d(j,i,t) = |f(j,i,0)-f(j,i,t)|, 0<=j<m, 0<=i<n, 1<=t<=T

### 모션 벡터와 광류
<목표> 움직이는 물체는 연속 프레임에 명암 변화를 발생하니, 이를 분석해 역으로 물체의 모션 정보 확인     
</br>   
광류 optical flow : 화소별로 모션벡터 motion vector를 추정해 기록한 맵     
== 이전 프레임과 현재 프레임의 차이를 이용하고 픽셀값과 픽셀과의 관계를 통해 각 픽셀의 이동motion을 계산해 추출 

![image](https://user-images.githubusercontent.com/109460178/230829666-2ef6ce34-2e00-46bb-948b-df0fa05be354.png)

출처 : https://gaussian37.github.io/vision-concept-optical_flow/

모션벡터를 추정하는 알고리즘은 물체가 이동, 회전, 크기변환을 하며, 환경이 조명변화와 잡음을 복합적으로 발생시켜 화소가 다음 순간에 어디로 이동했는지 확정하기는 어려움        
=>    
해결 방안으로 연속한 두 영상에서 같은 물체는 같은 명암으로 나타난다는 **밝기 향상성 brightness constancy** 조건 가정     
<br>
시간 간격이 짧다면 테일러 근사로 전개 -> 광류조건식 optical flow constrait equation 생성

![image](https://user-images.githubusercontent.com/109460178/230828050-20fdccc1-9f01-43b2-be85-932c4a7a70cb.png)

+ 각각 v,u는 dt동안 y,x방향으로 이동한 양으로 모션벡터에 해당
+ df/dy. df/dx는 각각 y,x방향의 명암 변화로서 에지검출에 사용한 에지연산자 사용
+ (영상의 y축 변화) x v + (영상의 x축 변화) x u + 영상의 t축 변화 = 0
+ 식은 하나지만 변수는 두개이므로 유일한 해를 확정할 수는 없지만 **이웃 화소 관계를 고려**하면 정확한 해 도출 가능

1. Lukas-Kanade 알고리즘   
  어떤 화소는 이웃 화소와 유일한 모션 벡터를 갖는다는 지역조건 사용    
  - 장점 | 연산량 적음  
  - 단점 | 정확도 낮음

![image](https://user-images.githubusercontent.com/109460178/230831582-fe7a564a-c7a4-4af6-a417-75fd5d38e3b2.png)

  1) 밝기향상성
  2) frame 간 움직임 적음 
  3) 픽셀 (y,x)를 중심으로 하는 윈도우 영역 N(y,x)의 optical flow 동일
   
![image](https://user-images.githubusercontent.com/109460178/230830533-6f5e25b4-1961-4f08-998d-0d57879caedc.png)

 
2. Horn-Schun 알고리즘           
  영상 전체에 걸쳐 서서히 변해야 한다는 광역조건 사용   
  - 장점 | 정확도 높음  
  - 단점 | 모든 픽셀을 계산하기에 속도 느림
  
  1) 밝기향상성
  2) frame 간 움직임 적음 
  3) optical flow 균일
  
  ![image](https://user-images.githubusercontent.com/109460178/230830562-a637a7ce-06cc-4a6d-997b-4414af0c0102.png)

3. Farneback 알고리즘              
  인접한 두 프레임 간의 움직임을 확장 다항식 기반으로 계산하는 dense optical flow의 한 종류
  - 장점 | 정확도 높음  
  - 단점 | 계산 과정이 복잡해 시간 오래 걸림
</br>
  `cv.calcOpticalFlowFarneback(	prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags	)`       
  - input 배열 이전 영상 
  - input 배열 현재 영상 
  - 출력 계산된 옵티컬 플로우     
  - 축소 비율      
  - 영상 개수      
  - 윈도우 크기     
  - 각 영상개수(levels)에서 알고리즘 반복 횟수              
  - 다항식 확장을 위한 이웃 픽셀 크기            
  - 가우시안 표준편차                
  - flags 
     - 0 : ?
     - cv2.OPTFLOW_USE_INITIAL_FLOW : 인풋 플로우를 초기 플로우의 근사치 사용
     - cv2.OPTFLOW_FARNEBACK_GAUSSIAN : 가우스 원사이즈x원사이즈 필터 사용                      
 
4. 희소 광류 추정을 이용한 KT 추척 알고리즘
지역 특징을 추적해 유리하도록 개조한 특징을 추출한 다음, 이들 특징ㅇ을 광류 정보를 이용해 추적하는 방식으로 동작               
희소 광류 sparse optical flow : 지역 특징으로 추출된 점에서만 모션 벡터 추정

  `p1,match,err=cv.calcOpticalFlowPyrLK(old_gray,new_gray,p0,None,**lk_params)`
   - old_gray : 이전 프레임 영상 
   - new_gray : 다음 프레임 영상
   - p0 : 이전 프레임의 코너 특징점, cv2.goodFeaturesToTrack()으로 검출
   - p1 : 다음 프레임에서 이동한 코너 특징점 = 새로 찾은 특징점 
   - match : 결과 상태 벡터, nextPts와 같은 길이, 대응점이 있으면 1, 없으면 0 = 매칭 성공 여부
   - err : 결과 에러 벡터, 대응점 간의 오차 = 매칭 오류

### 딥러닝 기반 광류 추정
광류는 매 화소마다 모션 벡터를 지정해야 하기에 참값 만들기 어려움

![image](https://user-images.githubusercontent.com/109460178/230842437-9317a45f-d37f-451c-b648-2e8d002ab477.png)

a. 자율주행차를 위해 제작한 KITTI 데이터셋이 제공하는 광류데이터       
   차량에 설치된 여러 장치를 통해 광류의 참값을 자동으로 레이블링     
   
b. 컴퓨터그래픽을 제작한 애니메이션 영화 장면에 광류의 참값을 레이블링한 Sintel 데이터 셋     
   컴퓨터그래픽 프로그램이 물체의 움직임에 대한 정보를 알고 있어 자동으로 레이블링 가능      
   
c. FlowNet이 자체제작한 Flying chairs 데이터셋
   실제 영상에 의자를 인위적으로 추가한 다음 자동으로 광류 계산해 레이블링

**분할결과를 활용한 광류 추정의 성능 개선하려는 시도 O**                          
자율주행 영상을 대상으로 알고리즘을 구상, 의미분할 대신 서로 다른 자동차 구분해주는 사례분할 적용

![image](https://user-images.githubusercontent.com/109460178/230843098-d2268209-b1f6-449d-8eaf-fa94f8fb43e7.png)


## 10.2 추적
KIT 지역 특징을 추적하기에 뚜렷한 특징점이 나타나지 않는 물체 추적 불가      
-> 추적을 위한 고전 알고리즘이 널리 활용되지 못하는 근본 이유               

검출이나 분석할 결과를 잘 활용하면 박스나 영역 단위로 물체 추적 가능하며 추적 대상이 어떤 물체인지 인식 가능           
<목적> 물체를 실시간으로 추적하는 알고리즘        

재식별 re-identification : 끊긴 추적을 매칭해 같은 물체로 이어주는 과정           
  추적할 물체의 개수에 따라
  - VOT Visual Object Tracking 시각 물체 추정
  - MOT Multiple Object Tracking 다중 물체 추정 : 영상에 있는 여러 물체를 찾아야 하는데 첫 프레임에서 물체 위치를 지정해주지 않고 추적할 물체의 부류 지정            

  ![image](https://user-images.githubusercontent.com/109460178/230844957-27b033d5-2837-45cd-bd39-414d85a8dab2.png)

### 성능 척도 
프레임 간의 연관성까지 고려해야 하기에 분할보다 복잡     
MOT (MOT Accuracy)

![image](https://user-images.githubusercontent.com/109460178/230845440-5e7481e4-3dbd-4fc3-9528-0d27006f9422.png)

모든 프레임에서 아래의 값들 count    
- GT_t : t 순간에서 참값 
- FN_t : t 순간에서 거짓 부정
- FP_t : t 순간에서 거짓 긍정 
- IDSW_t : 번호 교환 오류 개수   
이들 값은 매칭 알고리즘에 따라 정해지며 IoU가 임곗값을 넘으면 매칭


![image](https://user-images.githubusercontent.com/109460178/230845985-fef60aff-d6c8-4a32-a2d6-d6ed90d970e8.png)

- 비디오 길이 T=7
- 점선 궤적은 참값 (1-6 순간 지속하는 궤적(1), 5-7 순간 지속하는 궤적(2))
- IoU 임곗값 = 0.5

+ 순간1 : 빨간색 임곗값 넘지 못해 매칭 불발 => FN, FP
+ 순간2 : 매칭 발생 => TP
+ 순간3 : 빨간색 매칭, 파란색 불발 => TP, FP
+ 순간4 : 빨간색 매칭, 파란색 불발 => TP, FP
          GT가 파란색과 더 가까우나 이전 순간에서 빨간색과 매칭되었기 때문에 성능평가프로그램이 이력을 고려해 결정
+ 순간5 : 빨간색 임곗값 넘지 못해 매칭 불발, 파란색 매칭, 노란색 매칭 => FP, TP, TP    
          궤적(1)입장에서 다른 물체와 쌍이 맺어져 빨간색과 파란색의 번호교환 발생
+ 순간6 : 빨간색 매칭, 노란색 불발 => TP, TP
+ 순간7 : 노란색 매칭 => TP

![image](https://user-images.githubusercontent.com/109460178/230855826-ff72cf15-f929-415f-b21d-4fba9c8d1dde.png)

단점 : 쌍 맺기 성공률이 낮더라도 검출 성공률 높으면 좋은 점수 부여
















## 10.3 MediaPipe를 이용해 비디오에서 사람 인식
비디오에 나타난 사람을 인식하는 여러 방법 설명



## 10.4 자세 추정과 행동 분류
라이브러리로 얼굴과 손을 검출하고 사람의 자세를 추정하는 프로그래밍 실습
