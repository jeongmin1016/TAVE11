# 2주차 요약본

[연습문제_홀수](https://www.notion.so/_-350e46b731204d1ba8e52460de278060)

# Chap4. 에지와 영역

## 4.1 에지영역

### 에지

물체 경계에 위치한 점

- 에지 검출 : 에지에 해당 화소 찾기
- 에지 향상 : 에지 더 잘보이도록 하기 위해 에지와 배경 간의 대비 증가
- 에지 추적 : 에지 따라가기
- 에지를 완벽하게 검출해 물체의 경계를 폐곡선으로 따낼 수 있다면 분할 문제 해결
- 특성이 크게 다른 화소에 집중하는 방식
- > 물체의 위치 모양 크기 에 대한 정보 찾기 가능

### 영역

특성이 비슷한 화소를 묶는 방식

에지 검출 알고리즘 : 물체 내부는 명암이 서서히 변하고 경계는 급격히 변하는 특성 활용

### 4.1.1 영상의 미분

미분 : 변수의 x 값이 미세하게 증가했을 때 함수 변화량 측정, 에지 추출 방법

- 영상을 (x, y) 변수의 함수로 간주했을 때, 이 함수의 1차 미분(1st derivative) 값이 크게 나타나는 부분을 검출

디지털 영상 미분

- 미분은 기본적으로 연속적인 공간에서 적용, but 영상은 연속공간이 아닌 이산 공간이기에 근사화한 것으로 적용
- 영상 f에 미분 적용해 f' -> 영상에서 가장 작은 단위가 1이며 (-1,1)로 컨볼루션
- 필터 u(에지 연산자)로 컨볼루션하여 구현
- 명암을 미분해 튀어나온 부분이 에지

### 4.1.2 에지 연산자

- 명암 변화 x = 0
- 명암 변화 o = 3
- 에지 검출은 모두 명암 변화에만 의존, 두 인접한 물체가 비슷한 명암을 가져 명암 변화가 적은 경우 경계에서 에지 발생X

컨볼루션의 값은 f 원래 영상의 부호를 의미한다 !?

물체 경계를 지나면서 명암값이 커지면 미분값 양수, 작아지면 음수

램프에지 : 계단 모양이 아닌, 명암이 몇화소에 걸쳐 변화, 정확한 에지의 위치 찾기 어려움

- --- 에지 검출 과정 ---
두꺼운 에지에서 위치 찾기 적용

1차미분 : 원래 영상의 변화량이 1차 미분값
에지 발생여부와 에지가 어떤 방향을 향하는지 알 수 있음
봉우리 찾기

2차미분 : 1차 미분값의 변화량이 2차 미분값
영교차 찾기

![https://user-images.githubusercontent.com/109460178/227122703-6325016c-3126-446e-86b0-d14d2499b2ee.png](https://user-images.githubusercontent.com/109460178/227122703-6325016c-3126-446e-86b0-d14d2499b2ee.png)

- 봉우리 : 봉우리의 두께가 1이므로 계단 에지만 존재하는 경우 에지 찾기 간단
- 영교차 : 왼쪽과 오른쪽에 부호가 다른 반응 발생, 자신은 0을 갖는 위치

### 1차 미분에 기반한 에지 연산자

미분에 기반한 2차원 에지 연산자는 크기 확장시 잡음을 흠수해 더 좋은 성능 보임
-> 잡음으로 인해 스무딩이 필요해 2차원 연산저 적용

- 프로윗
장점 : 돌출된 값을 잘 평균화
단점 : 값이 일렬로 나열되어 있어 대각선보다 수평 수직에 놓인 에지에 민감
- 소벨
장점: 돌출된 값 잘 평균화, 모든 방향 에지 검출 가능, 잡음에 강함
단점 : 대각선 방향에 놓인 에지 민감

실제영상에서 구한 그래디언트 크기와 방향은 프레윗과 소벨을 적용한 결과 영상

- 에지 강도 : 크기 = 픽셀 값의 차이 정도, 변화량 -> 에지 가능성 나타냄
- 에지 방향 : 방향 = 값이 급격히 증가하는 방향 -> 에지 진행 방향 나타냄

![https://user-images.githubusercontent.com/109460178/227124583-f145bb2f-f6e8-4783-9ab3-9f3d2f055de9.png](https://user-images.githubusercontent.com/109460178/227124583-f145bb2f-f6e8-4783-9ab3-9f3d2f055de9.png)

각각 프로윗 소벨 적용한 값을 기반으로 에지 방향 구함

```python
# 4-1 에지 검출
import cv2 as cv

img=cv.imread('soccer.jpg')
gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY) # 명암으로 변화

# 소벨 연산자 적용
grad_x=cv.Sobel(gray,cv.CV_32F,1,0,ksize=3)	# 결과 영상 32비트 실수 맵 저장, (1,0) x 방향 연산자, 3*3 크기 사용
grad_y=cv.Sobel(gray,cv.CV_32F,0,1,ksize=3) # y 방향 연산자

# 음수가 포함된 맵에 절대값을 취해 양수 영상으로 변환
sobel_x=cv.convertScaleAbs(grad_x)	# convertScaleAbs : 부호없는 8비트형 맵을 형성해 크기가 0보다 작으면 0, 255넘으면 255
sobel_y=cv.convertScaleAbs(grad_y)

# 에지 강도 계산
edge_strength=cv.addWeighted(sobel_x,0.5,sobel_y,0.5,0)	# sobel_x * 0.5 + sobel_y * 0 + 0
# sobel_x 와 sobel_y 가 같은 데이터 형이면 결과영상 같은 데이터 형
# 다른 데이터 형이면 결과영상 에러 

# 결과영상 윈도우 디스플레이
cv.imshow('Original',gray)
cv.imshow('sobelx',sobel_x)
cv.imshow('sobely',sobel_y)
cv.imshow('edge strength',edge_strength)

cv.waitKey()
cv.destroyAllWindows()
```

```python
cv2.Sobel(src, ddepth, dx, dy, dst=None, ksize=None, scale=None, 
delta=None, borderType=None) -> dst
```

- 입력 영상
- 출력영상 데이터 타입으로 -1이면 입력 영상과 같은 데이터 타입 사용
- x방향 미분 차수
- y방향 미분 차수
- 출력영상행렬
- 커널 크기
- 연산결과에 추가적으로 곱한 값으로 기본값 1
- 연산 결과에 추가적으로 더할 값으로 기본값0
- 가장자리 픽셀 확장방식

## 4.2 캐니 에지

1. 블러링 통한 노이즈 제거 : 5*5 크기의 가우시안 필터 적용해 불필요 잠음 제거
2. 그래디언트 계산 : 기울기의 크기와 방향 계산

**3. 비최대치 억제**
현재 화소가 이웃하는 화소들보다 크면 보존, 그렇지 않으면 에지가 아닌 것으로 간주 == 최대가 아니면 억제
에지 방향에 수직인 두 이웃 화소의 에지 강도 비교

**4. 이력 임계값으로 에지 결정**

실제 에지가 아닌데 에지로 검출된 화소인 거짓 긍정을 줄이기 위해 임계값 사용

두개의 임계값 T(high), T(low)를 사용해서 에지의 이력을 추적하여 에지를 결정하는 방법

![https://user-images.githubusercontent.com/109460178/227133915-5b6112c4-c68b-4029-bb13-bbbd3dd99b45.png](https://user-images.githubusercontent.com/109460178/227133915-5b6112c4-c68b-4029-bb13-bbbd3dd99b45.png)

- 에지 추적은 T_high를 넘는 화소에서 시작, 추적 도중에는 T_low 적용
- 이웃 화소가 추적이력이 있으면 자신은 신뢰도가 낮더라도 에지로 간주
- 추적 시작하지 않은 이웃 화소들을 대상으로 T(low)보디 큰 화소 에지 결정

```python
# 4-2 캐니 에지
# import cv2 as cv

img=cv.imread('soccer.jpg')	# 영상 읽기
gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)

canny1=cv.Canny(gray,50,150)	# Tlow=50, Thigh=150으로 설정
canny2=cv.Canny(gray,100,200)	# Tlow=100, Thigh=200으로 설정

cv.imshow('Original',gray)
cv.imshow('Canny1',canny1) # 에지 강도가 작은 화소도 추적 가능하나 잡음 발생
cv.imshow('Canny2',canny2) # 임계값 높으면 에지강도가 큰 화소만 추적해 더 작은 에지 발생

cv.waitKey()
cv.destroyAllWindows()
```

## 4.3 직선 검출

에지 화소는 1, 에지가 아닌 화소 0**에지를 연결해 경계선으로 변환하고 경계선을 직선으로 변환** 

→ **물체 표현이나 인식에 유리**

1) 에지 맵에서 경계선 검출 

2) 길이가 임계값 이상인 경계선만 취함

```python
# 4-3 직선 검출
import cv2 as cv
import numpy as np

img=cv.imread('soccer.jpg')	 # 영상 읽기
gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)
canny=cv.Canny(gray,100,200) 

contour,hierarchy=cv.findContours(canny,cv.RETR_LIST,cv.CHAIN_APPROX_NONE) # 경계선 정보 검출

lcontour=[]   
for i in range(len(contour)):
    if contour[i].shape[0]>100:	# 길이가 100보다 크면 
        lcontour.append(contour[i])
  
# 외곽선 그리기
cv.drawContours(img,lcontour,-1,(0,255,0),3)
             

cv.imshow('Original with contours',img)    
cv.imshow('Canny',canny)    

cv.waitKey()
cv.destroyAllWindows()
```

### findCountours 사용

- 경계선 찾을 에지 영상
- 구멍이 있는 경우 바깥쪽 경계선과 그 안에 구멍의 경계선을 계층적으로 찾는 방식 지정
-> cv.RETR_LIST : 맨 바깥쪽 경계선 찾기
- 경계선 표현 방식 지정
-> cv.CHAIN_APPROX_NONE : 모든 점 기록 / cv.CHAIN_APPROX_SIMPLE : 직선에 대해 양 끝점만 기록 / cv.CHAIN_APPROX_TC89_L1 & cv.CHAIN_APPROX_TC89_KCOS : Teh-Chin 알고리즘으로 굴곡이 심한 점 찾아 기록
    - contours: 검출된 외곽선 좌표
    • hierarchy: 외곽선 계층 정보

### drawCountours 사용

왔다갔다 반복되는 부분이 있어서, 50이상의 경계선을 검출하고 싶다면 한계 100
-> 함수는 시작점부터 끝점까지 추적한 다음 역추적하여 시작점으로 돌아와 경계선 표현하기 때문

- 경계선 그려넣을 영상
- 경계선
- 모든 경계선 그리기 (-1), 해당 번호에 해당하는 경계선 하나만 그리기 (양수)
- 색
- 두께

### 허프변환

잡음 발생 : 에지가 자잘하게 끊겨 나타나는 경우 발생 -> **"허프변환"** 적용하면 끊긴 에지 모아 선분 or 원 등 검출

- 직선의 방정식 이용
1. 공간 (x,y)에 위치한 직선 y = ax + b
2. b = -ax + y 로 변경 가능하며 이는 a, b를 변수로 취급, 새로운 공간 (a,b) 생성
3. 1)에서 점이였던 것이 공간이 변하며 직선으로 변경
4. 새로운 공간에서의 두개의 직선이 만난다면 이 만나는 점은 원래 공간에서 두 점을 지나는 직선의 기울기와 y절편

![https://user-images.githubusercontent.com/109460178/227456183-f429d1da-7a58-40ba-acb5-d4d970bd96f5.png](https://user-images.githubusercontent.com/109460178/227456183-f429d1da-7a58-40ba-acb5-d4d970bd96f5.png)

- > (a,b) 공간에서 **두 직선이 만나는 점은 투표**로 알아냄
-> 직선이 지나지 않음 0, 직선이 지남 1, 두 직선이 만남 2

발생 시 고려사항
조건1) fact, 많은 점들이 있고 점들이 완전히 일직선을 이루진 않음
-> (a,b)공간을 이산화해 해결 : 2차원 누적 배열 v 생성, v를 0으로 초기화한 다음 각각 직선은 자신이 지나는 모든 칸에 1만큼 투표

조건2) 투표가 이뤄진 누적 배열에 잡음 많음
-> 비최대억제로 잡음이 많은 상태에서 한 점 결정

![https://user-images.githubusercontent.com/109460178/227457314-9d0d99c8-2bd1-4697-9101-273d619b2b93.png](https://user-images.githubusercontent.com/109460178/227457314-9d0d99c8-2bd1-4697-9101-273d619b2b93.png)

:: 비최대억제로 극점 3개 남았는데 임계값을 적용해 노란색점 2개가 최종

조건3) y = ax + b, 기울기 a가 무한대인 경우 투표 불가
-> 극좌표에서 직선의 방정식 표현하는 식으로 해결

![https://user-images.githubusercontent.com/109460178/227457688-c7c771ff-f27f-4def-bd21-29f09699231f.png](https://user-images.githubusercontent.com/109460178/227457688-c7c771ff-f27f-4def-bd21-29f09699231f.png)

- 한계점)
직선의 양끝점 알려주지 못함
-> 비최대억제과정에서 극점을 형성한 화소를 찾아 가장 먼 곳에 있는 두 화소 계산하는 추가 과정 필요
ex) 원 검출하기 위해 원의 방정식 이용

```python
cv.HoughCircles(gray,cv.HOUGH_GRADIENT,1,200,param1=150,param2=20,minRadius=50,maxRadius=120) # 원 검출 허프변환
```

- 명암영상에서 원 검출해 중심과 반지름 저장한 리스트 반환
- 여러 변형 알고리즘 중 하나 지정
-> cv.HOUGH_GRADIENT : 에지 방향 정보 추가 사용
- 누적 배열의 크기 지정
-> 1 : 입력 영상과 동일 크기 사용
- 원 사이의 최소 거리 지정, 작을수록 많은 원 검출
- 캐니 에지 알고리즘이 사용하는 임계값 T_high
- 비최대 억제 적용할 때 사용하는 임계값
- 원의 최소 반지름
- 원의 최대 반지름

허프변환 문제점 

이상치가 섞여있을 수 있는데 허프변환은 모든 점에 동일한 투표기회를 제공하기 때문에 누적 배열에 잡음이 많이 발생하는 원인

### 최소평균제곱오차 알고리즘

허프변환과 유사함 (이상치에 민감함)
모든 점을 대상으로 오류 계산하고 최소 오류 범하는 직선 찾기

![https://user-images.githubusercontent.com/109460178/227461993-a22cd671-d128-44ee-94ce-a2e29a92c879.png](https://user-images.githubusercontent.com/109460178/227461993-a22cd671-d128-44ee-94ce-a2e29a92c879.png)

아웃라이어의 영향으로 실제에서 벗어난 직선 찾음, 모든 샘플이 동등한 자격으로 오류 계산 참여

문제점 해결을 위해 아래 알고리즘 적용

### 강인한 추정

아웃 라이어를 걸러내는 과정을 가진 추정 기법
=> 중앙값을 이용해 아웃라이어를 배제하여 추정하기 때문

### RANSAC

인라이어와 아웃라이어가 섞여 있는 상황에서 인라이어를 찾아 최적 근사하는 기법

1. 랜덤하게 두 점 서택, 두점을 지나는 직선 계산
2. 일정한 양의 오차 t를 허용해 직선에 일치하는 점의 개수 count
3. 개수가 임곗값 d를 넘지 못하면 가능성 X -> 버림
4. 3번 통과시, 해당 점의 개수로 최적 직선 추정하고 추정 오류가 임계값 e보다 작으면 후보군 추가 아니면 out
5. 3번과 4번의 반복으로 후보군에서 최적 찾기

![https://user-images.githubusercontent.com/109460178/227463259-04303c7b-e2f2-4253-ac4d-81fd25a37a28.png](https://user-images.githubusercontent.com/109460178/227463259-04303c7b-e2f2-4253-ac4d-81fd25a37a28.png)

a. d=5라면 해당점이 3개이기에 out
b. d=5라면 해당점이 7이기에 ok, 그러나 7개로 최적 직선 추정하고 추정 오류가 임계값 e보다 작으면 후보군 추가 아니면 out

장점 : 반복횟수가 많아 직선 찾을 가능성 up
단점 : 시간이 더 걸려 적절한 값 설정 필요

## 4.4 영역분할

영역 분할 : 물체가 점유한 영역을 구분하는 작업

의미분할 : 의미 있는 단위로 분할하는 방식

이진화 알고리즘, 군집화 알고리즘 를 적용해 좋은 분할 성능을 얻을 수 있음

### 워터셰드

비가 오면 오목한 곳에 웅덩이가 생기는 현상 모방하는 연산

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/85dd70da-b600-4065-a751-e0c4649e40df/Untitled.png)

(b)에서 낮은 곳부터 물을 채우는 연산을 반복하면 (c)와 같이 서로 다른 **웅덩이**를 찾을 수 있음

→ 이러한 웅덩이를 **영역으로 간주**

### SLIC 알고리즘

슈퍼 화소 알고리즘 (영상을 작은 영역으로 분할해 다른 알고리즘 입력으로 사용하며, 이 영역은 화소보다 크지만 물체보다 작아 슈퍼화소라 불림) 중 하나

- 화소 5차원 벡터표현 (R, G, B, x, y) : 색상 나타내는 3개의 값 + 위치 나타내는 2개의 값

1. 입력영상에서 k개 화소를 군집 중심으로 지정
    
    등간격 패치로 분할한 다음 패치 중심을 군집 중심으로 간주
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5a5684ee-e419-4924-8a7e-e6f94519317d/Untitled.png)
    
2. 군집 중심이 물체 경계에 놓이는 일 방지하기 위해 그레이디언트가 가장 낮은 이웃 화소로 이동
3. 화소를 가장 가까운 군집 중심에 할당하는 단계와 군집 중심을 갱신하는 단계 반복
    - 화소 할당 : 화소 각각에 대해 주위 4개 군집 중심과 자신까지 거리를 계산해 가장 유사한 군집 중심 할당
    - 중심 갱신 : 각 군집 중심은 자신에게 할당된 화소를 평균해 군집 중심 갱신
4. 모든 군집 중심의 이동량의 평균을 구하고 평균이 임계치보다 작으면 수렴했다고 판단. 
    
    → 알고리즘 stop
    
    ```python
    # 4-4
    import skimage # 입력 영상을 슈퍼화소로 분할하기에 slic 함수가 사용하기 편리
    import numpy as np
    import cv2 as cv
    
    img = skimage.data.coffee() # 내부 coffee 영상 읽어 객체 저장
    cv.imshow('Coffee image',cv.cvtColor(img,cv.COLOR_RGB2BGR))
    
    slic1 = skimage.segmentation.slic(img,compactness=20,n_segments=600) # 슈퍼 화소 분할 수행
    sp_img1 = skimage.segmentation.mark_boundaries(img,slic1) # 객체 분할 정보 img 영상에 표시, 결과를 sp_img1 객체에 저장
    sp_img1 = np.uint8(sp_img1*255.0) # 표현 type을 0-255사이로 변환하고 unit8형으로 변환
    
    # 위와 동일, 파라미터만 수정
    slic2=skimage.segmentation.slic(img,compactness=40,n_segments=600) 
    sp_img2=skimage.segmentation.mark_boundaries(img,slic2)
    sp_img2=np.uint8(sp_img2*255.0)
    
    cv.imshow('Super pixels (compact 20)',cv.cvtColor(sp_img1,cv.COLOR_RGB2BGR))
    cv.imshow('Super pixels (compact 40)',cv.cvtColor(sp_img2,cv.COLOR_RGB2BGR))
    
    cv.waitKey()
    cv.destroyAllWindows()
    ```
    

`skimage` 은 numpy 배열로 영상 표현하며 , RGB 순서로 저장하기에 `cvtColor`함수로 BGR로 변환해 출력

```python
skimage.segmentation.slic(img,compactness=20,n_segments=600)
```

- 슈퍼 화소 분할할 영상
- 슈퍼화소의 모양 조절 : **값이 클수록 네모에 가까운 모양** 형성 but 슈퍼 화소의 모양을 조절
- 슈퍼 화소의 개수 지정 → 알고리즘에서 사용한 **k**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/93960c32-a251-4a5e-a3fc-a3ea495e1334/Untitled.png)

위와 같이 값이 클수록 네모에 가깝게 유지되지만 슢화소의 색상 균일성 down

### 최적화 분할

영상을 그래프로 표현하고 분활을 최적화 문제로 풀이

→ 지역적 명암 변화를 보며 전역적 정보를 같이 고려

- 영상을 그래프로 표현할 때 슈퍼화소를 노드로 취하면 노드 개수를 효율적으로 감소할 수 있음
- 두 노드 v_p, v_q 를 연결하는 에지 가중치로는 **유사도 s_pq** 사용
- f(v)는 v에 해당하는 화소의 색상과 위치를 결합한 벡터
- “v_q 가 neighbor(v_p)에 속한다면” ::  v_q와 v_p가 8-이웃을 이루거나 둘 사이의 거리가 사용자가 지정한 값 r 이내면 참

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f6790e6c-e399-4bc5-bd9a-88cd73f15b14/Untitled.png)

**정규화 절단 알고리즘**

정규화 절단 : 화소를 노드로 취하고, f(v)로 5차원 벡터 사용, 유사도를 에지 가중치로 사용

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/99a8c90e-20a1-4f18-9c27-8bae228972f3/Untitled.png)

원래의 영역을 두개로 분할할 때 *cut* 적용, 영역 분할의 좋은 정도 측정해주는 목적함수

→ 두 영역이 클수록 둘 사이에 에지가 많아 덩달이 증가

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ee47bccd-7aab-4d51-94a6-70a5acce2b63/Untitled.png)

cut 을 정규화해 영역의 크기에 **중립**이 되도록 함

→ ncut이 작을수록 좋은 분할이므로 최소화 문제

```python
import skimage
import numpy as np
import cv2 as cv
import time

coffee=skimage.data.coffee()

start=time.time() # 분할하는 데 걸리는 시간 측정
slic=skimage.segmentation.slic(coffee,compactness=20,n_segments=600,start_label=1) # 슈퍼화소로 분할

g=skimage.future.graph.rag_mean_color(coffee,slic,mode='similarity') 
ncut=skimage.future.graph.cut_normalized(slic,g)	# 정규화 절단
print(coffee.shape,' Coffee 영상을 분할하는데 ',time.time()-start,'초 소요') # 시간 측정

marking=skimage.segmentation.mark_boundaries(coffee,ncut)
ncut_coffee=np.uint8(marking*255.0)

cv.imshow('Normalized cut',cv.cvtColor(ncut_coffee,cv.COLOR_RGB2BGR)) 

cv.waitKey()
cv.destroyAllWindows()
```

`rag_mean_color` 슈퍼화소를 노드로 사용하고 ‘similarity’ 를 에지 가중치로 사용한 그래프를 구성

`mark_boundaries` 원래 영상에 화소에 영역의 번호를 부여한 맵을 이용해 영역 경계를 표시

## 4.5 대화식 분할

### 능동 외곽선 알고리즘

사용자가 물체 내부에 초기 곡선을 지정하면 곡선을 점점 확장하며 물체 외곽선 접근

- 곡선이 꿈틀대며 에너지가 최소인 상태를 찾아가기에 스네이크라는 별명 생성
    
    → 곡선 구현 방법 필요
    

$$
g(l) = (y(l), x(l))
$$

g(l) : 2차원 상의 곡선이므로 위와 같이 표현

l : 매개변수, [0,1] 범위의 실수이지만 디지털 공간은 이산공간이기에 *0, 1, 2 .., n* 으로 표현

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ce707ee5-39eb-4eba-b158-8d6ee0c6c7c2/Untitled.png)

에너지를 최소로 하는 최적의 곡선을 찾는 최적화 문제

1. 사용자가 입력 받아 초기 곡선 설정 
2. 자신과 8-이웃을 포함한 9개의 점에 대해 곡선 에너지 E 도출
3. 에너지가 최소점으로 이동하고 이동량 증가 
4. 곡선의 이동량이 임계치보다 작으면 수렴했다고 간주하고 최적 곡선 도출

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/14a2b7f3-46c2-47d0-88e1-ee695163b1e3/Untitled.png)

### GrabCut

사용자가 붓칠한 정보를 이용해 물체 분할

```python
import cv2 as cv 
import numpy as np

img=cv.imread('soccer.jpg')	# 영상 읽기
img_show=np.copy(img)		# 붓 칠을 디스플레이할 목적의 영상

# 사용자가 붓칠에 따라 물체인지 배경인지에 대한 정보를 기록할 배열 생성
mask=np.zeros((img.shape[0],img.shape[1]),np.uint8) 
mask[:,:]=cv.GC_PR_BGD		# 모든 화소를 배경일 것 같음으로 초기화

BrushSiz=9				# 붓의 크기
LColor,RColor=(255,0,0),(0,0,255)	# 파란색(물체)과 빨간색(배경)

def painting(event,x,y,flags,param):
    if event==cv.EVENT_LBUTTONDOWN:   
        cv.circle(img_show,(x,y),BrushSiz,LColor,-1)	# 왼쪽 버튼 클릭하면 파란색
        cv.circle(mask,(x,y),BrushSiz,cv.GC_FGD,-1)

    elif event==cv.EVENT_RBUTTONDOWN: 
        cv.circle(img_show,(x,y),BrushSiz,RColor,-1)	# 오른쪽 버튼 클릭하면 빨간색
        cv.circle(mask,(x,y),BrushSiz,cv.GC_BGD,-1)

    elif event==cv.EVENT_MOUSEMOVE and flags==cv.EVENT_FLAG_LBUTTON:
        cv.circle(img_show,(x,y),BrushSiz,LColor,-1)# 왼쪽 버튼 클릭하고 이동하면 파란색
        cv.circle(mask,(x,y),BrushSiz,cv.GC_FGD,-1)

    elif event==cv.EVENT_MOUSEMOVE and flags==cv.EVENT_FLAG_RBUTTON:
        cv.circle(img_show,(x,y),BrushSiz,RColor,-1)	# 오른쪽 버튼 클릭하고 이동하면 빨간색
        cv.circle(mask,(x,y),BrushSiz,cv.GC_BGD,-1)

    cv.imshow('Painting',img_show)

    
cv.namedWindow('Painting')
cv.setMouseCallback('Painting',painting)

while(True):				# 붓 칠을 끝내려면 'q' 키를 누름
    if cv.waitKey(1)==ord('q'): 
        break

# ----------------- GrabCut 적용하는 코드 -----------------
background=np.zeros((1,65),np.float64)	# 배경 히스토그램 0으로 초기화
foreground=np.zeros((1,65),np.float64)	# 물체 히스토그램 0으로 초기화

# 실제 분할시도
cv.grabCut(img,mask,None,background,foreground,5,cv.GC_INIT_WITH_MASK)
mask2=np.where((mask==cv.GC_BGD)|(mask==cv.GC_PR_BGD),0,1).astype('uint8')
grab=img*mask2[:,:,np.newaxis]
cv.imshow('Grab cut image',grab)  

cv.waitKey()
cv.destroyAllWindows()
```

```python
cv.grabCut(img,mask,None,background,foreground,5,cv.GC_INIT_WITH_MASK)
```

- 원본 영상
- 사용자가 지정한 물체와 배경정보를 가진 맵
- 관심영역을 지정하는 ROI :  None - 전체 영상을 대상으로 하라고 지시
- 배경
- 물체 히스토그램
- n 번 반복
- 배경과 물체를 표시한 맵 사용

→ 여러번 반복해 정교하게 물체 오려내는 실험 go

## 4.6 영역 특징

불변성 : 변환을 해도 특징의 값이 변하지 않음

↔ 등변성 : 특징이 어떤 변환에 대해 따라 변화함

회전과 축소에 불변인 특징을 사용해야 구분해낼 수 있음

### 영역 R의 모멘트 정의

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b4b81d75-8679-4d44-90f0-b316ee1f56fd/Untitled.png)

(y,x) : 영역 R에 속하는 화소

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3da4d552-7b8a-416d-b32c-ee664668417e/Untitled.png)

(4.15) 중심 모멘트로부터 열분산, 행분산, 열행분산 모두 구할 수 있고, 주축 방향 도출 가능 

(4.17)에 해당하는 **크기불변** 도출 가능 → 영역의 둘레, 둥근 정도 측정 가능

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/aed5c396-8eba-43d9-8cbb-fcb160ba219f/Untitled.png)

### 텍스처

일정한 패턴의 반복

텍스처가 세밀하면 많은 에지 발생, 거칠면 적게 발생하는 성질 이용

busy는 에지 화소 수를 전체 화소 수로 나눠 세밀함 측정

**LBP** Local Binary Patern : 중심 화소와 주위 화소의 명암값을 비교해 텍스처 구함

→ 작은 명암 변화에 민감 

**LTP** Local Ternary Patern : LBP 단점 보완

### 이진영역의 특징을 추출하는 함수 사용

```python
import skimage
import numpy as np
import cv2 as cv

orig=skimage.data.horse()
img=255-np.uint8(orig)*255 # 말 영역은 255, 배경은 0
cv.imshow('Horse',img)

# 물체 경계선 추출
contours,hierarchy=cv.findContours(img,cv.RETR_EXTERNAL,cv.CHAIN_APPROX_NONE)

img2=cv.cvtColor(img,cv.COLOR_GRAY2BGR)		# 컬러 디스플레이용 영상
cv.drawContours(img2,contours,-1,(255,0,255),2) # 경계선 표시 (영상,경계선,경계선 모두 표시, 색깔, 선 두께)
cv.imshow('Horse with contour',img2)

contour=contours[0]

m=cv.moments(contour)				# 모멘트 추출해 저장  
area=cv.contourArea(contour) # 경계선으로 둘러싸인 영역의 면적 계산
cx,cy=m['m10']/m['m00'],m['m01']/m['m00'] # 중점
perimeter=cv.arcLength(contour,True) # 둘레의 길이 계산
roundness=(4.0*np.pi*area)/(perimeter*perimeter) # 둥근정도
print('면적=',area,'\n중점=(',cx,',',cy,')','\n둘레=',perimeter,'\n둥근 정도=',roundness)

img3=cv.cvtColor(img,cv.COLOR_GRAY2BGR)		# 컬러 디스플레이용 영상

contour_approx=cv.approxPolyDP(contour,8,True)	# 직선 근사
cv.drawContours(img3,[contour_approx],-1,(0,255,0),2)

hull=cv.convexHull(contour)			# 볼록 헐
hull=hull.reshape(1,hull.shape[0],hull.shape[2])
cv.drawContours(img3,hull,-1,(0,0,255),2)

cv.imshow('Horse with line segments and convex hull',img3)

cv.waitKey()
cv.destroyAllWindows()
```

# Chap 5. 지역 특징

## 5.1 발상

대응점 문제 : 두 영상에서 특징점을 추출하고 매칭을 통해 해당하는 특징점 쌍 찾기

아래 조건을 만족해야 대응점 문제를 푸는 데 유용 = 특징점 검출

충돌되는 부분이 있기에 적절히 조절이 필요

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5e96b4c3-6ce8-4f43-b185-25a915e96ba0/Untitled.png)

## 5.2 이동과 회전 불변한 지역 특징

### 모라벡 알고리즘

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/988eca46-c474-42d3-b2b8-2637fbd53b95/Untitled.png)

1. 여러 방향으로 색상 변화가 있어 찾기 쉬움 
2. 어느 방향으로도 밝기 변화가 미세해 찾기 어려움

제곱차의 합 SSD : 영상의 특징점 중 어느 점이 찾기 쉬운지, 어려운지 **찾기 쉬운 정도를 측정**하는 데 쓸 수 있음, 각 화소 v와 u를 각각 -1,0,1로 변화시켜 맵 생성

단점 : 현실적이지 않고 실제 영상은 단순하지 않기에 상하좌우 이웃만 보고 점수 부여는 한계

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/878eff24-168e-452b-a4cb-3c7ad0ff79fe/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b7ed4f3c-1c59-4d44-bd31-96f55c6c3de6/Untitled.png)

- 지역 특징으로 **좋은 정도 측정**
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/edb75b11-7433-48a6-8602-e23bbe4407c7/Untitled.png)
    
1. 원래 영상에서 모든 방향으로 변화가 있어 S 맵에서 8이웃이 모두 큰 값 보유
    
    → 지역 특징으로 손색 없음 
    
2. 원래 영상에서 수직 방향으로 변화가 없고 수평 방향으로 변화가 있음, S맵에서 상하에 위치한 이웃 작은 값, 좌우 위치한 이웃 큰 값
    
    → 지역 특징으로 부족
    
3. 모든 방향에서 변화가 없어 S 맵의 모든 요소가 0 
    
    → 지역 특징으로 자격 X
    

### 해리스 알고리즘

1. 가중치 제곱차의 합 (잡음 대처를 위해 가우시안 G 추가 적용) 
2. 테일러 확장까지 따라 각 x,y 방향의 미분값도 적용

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/026d0e73-e180-468b-8c1b-0e206c13d258/Untitled.png)

이후 컨볼루션 연산자 이용해 가중치 제곱차의 합은 행렬 A 도출 가능 

행렬 A : 2차 모멘트 행렬

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9a3edbe8-c83d-47a1-906b-8912478a89fb/Untitled.png)

1. **실수**의 가중치 제곱차의 합 도출 가능 
2. 어떤 화소 주위의 영상 구조를 표현하고 있어 행렬 A만 분석하면 **지역 특징여부 판단** 
    
    고유값 이용해 적용 가능, 상수 k = 0.04 설정이 적절
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/06f18be0-45c2-41de-a760-056f7927a431/Untitled.png)
    
    ![행렬의 고윳값 특징이용해서 행렬값으로 수식 대치](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/eb00b80e-8f56-4d43-abdb-69e3392b7dd6/Untitled.png)
    
    행렬의 고윳값 특징이용해서 행렬값으로 수식 대치
    
    → 맵을 생성하는 계산 과정 거치지 않아도 됨
    
3. 고윳값을 보고 **지역 특징으로 좋은 정도** 측정하는 기법 제안 
    - 모두 0에 가까우면 특징으로 가치 없음
    - 하나는 큰데 다른 하나는 작은 경우 한 방향으로만 변화 있음
    - 모두 고윳값이 크면 지역 특징으로 good

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/00a1e4dc-9f8f-4561-9372-03f29659e944/Untitled.png)

```python
import cv2 as cv
import numpy as np

img=np.array([[0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0],
              [0,0,0,1,0,0,0,0,0,0],
              [0,0,0,1,1,0,0,0,0,0],
              [0,0,0,1,1,1,0,0,0,0],
              [0,0,0,1,1,1,1,0,0,0],
              [0,0,0,1,1,1,1,1,0,0],
              [0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0],
              [0,0,0,0,0,0,0,0,0,0]],dtype=np.float32) # 입력영상 생성

# 미분 필터 생성
ux=np.array([[-1,0,1]])
uy=np.array([-1,0,1]).transpose()
# 가우시안 필터 생성
k=cv.getGaussianKernel(3,1)
g=np.outer(k,k.transpose())

# dy, dx 도출
dy=cv.filter2D(img,cv.CV_32F,uy)
dx=cv.filter2D(img,cv.CV_32F,ux)

dyy=dy*dy
dxx=dx*dx
dyx=dy*dx

gdyy=cv.filter2D(dyy,cv.CV_32F,g)
gdxx=cv.filter2D(dxx,cv.CV_32F,g)
gdyx=cv.filter2D(dyx,cv.CV_32F,g)

# 특징가능성 맵 도출
C=(gdyy*gdxx-gdyx*gdyx)-0.04*(gdyy+gdxx)*(gdyy+gdxx) 

# 비최대 억제 사용
for j in range(1,C.shape[0]-1):		
    for i in range(1,C.shape[1]-1):
        if C[j,i]>0.1 and sum(sum(C[j,i]>C[j-1:j+2,i-1:i+2]))==8: 
          # 극점이 되려면 C가 0.1보다 커야 하며 8개 이웃보다 커야 함
            img[j,i]=9			# 특징점을 원본 영상에 9로 표시
                
# 특징 가능성 맵 C 계산하는 데 필요한 미분 영상 출력                
np.set_printoptions(precision=2)
print(dy) 
print(dx) 
print(dyy) 
print(dxx) 
print(dyx) 
print(gdyy) 
print(gdxx) 
print(gdyx) 

print(C)					# 특징 가능성 맵 
print(img)					# 특징점을 9로 표시한 원본 영상 

# 화소 확인 가능하게 16배로 확대해 윈도우에 보임
popping=np.zeros([160,160],np.uint8)	
for j in range(0,160):
    for i in range(0,160):
        popping[j,i]=np.uint8((C[j//16,i//16]+0.06)*700)  

cv.imshow('Image Display2',popping)    
cv.waitKey()
cv.destroyAllWindows()
```

## 5.3 스케일 불변한 지역 특징

스케일 불변의 가능성을 증폭하게 하는 **스케일 공간 이론**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/948d3ef1-a80c-48e3-9c3d-38e51879f19d/Untitled.png)

1. 가까이부터 멀리까지 본 장면 표현 필요 
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cd9777ec-6200-467d-8422-f0c31050bdbc/Untitled.png)
    
    1-1. 거리가 멀어지면 세부 내용이 점점 흐려지는 현상 모방
    
    → 표준편차 키우면서 가우시안 필터로 입력 영상 **스무딩**하여 흐리지는 현상 시물레이션
    
    장점 : 표준편차를 연속된 값으로 조절할 수 있는 장점
    
    1-2. 거리가 멀어짐에 따라 물체의 크기가 작아지는 현상 모방
    
    → 영상의 크기를 반씩 줄인 영상을 쌓은 **피라미드 영상**으로 이 현상 시물레이션
    
    장점 : 연속 공간에서 유도한 수식과 알고리즘을 디지털 공간으로 변환해 사용 가능 
    

## 5.4 SIFT

스케일 불변의 가능성을 증폭하게 하는 **스케일 공간 이론** 중 하나 SIFT

- 세 단계에 거쳐 특징점 검출
- 각 단계는 최적의 검출 정확도 달성하고 계산을 최대한 감소하는 연산 사용

1단계 : 다중 스케일 영상 구축

가우시안 + 피라미드로 다중 스케일 영상 구성

방법 : 아래 여섯장은 원래 영상에 가우시안 스무딩 적용 ⇒ 이렇게 적용된 영상 **옥타브**

  아래 여섯장 옥타브0, 위 여섯장 옥타브 1

- 둘을 결합한 형태
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4f264295-a617-4df5-94c6-38865559e58d/Untitled.png)
    
- 옥타브 0
    - 아래 여섯장
    - 원래 영상을 표준편차1 = 1.6 으로 스무딩한 영상에서 출발, 컨볼루션 6번 수행 (표준편차가 스무딩 역할)
        
        표준편차_(n+1) = k*표준편차n (n = 1,2,3,4,5) 
        
        → n이 증가할수록 필터가 커서 시간이 오래 걸림
        
    
- 옥타브 1
    - 위쪽 여섯장
    - 원래 영상을 반으로 줄여 가우시안 적용해 생성
    - 옥타브 1의 첫번째 영상은 옥타브 0에서 3.2로 스무딩한 네번째 영상을 반으로 축소해 얻음
    

**옥타브 0에서 옥타브 1을 얻는 과정을 반복 적용해 옥타브 2, 3, 4, 만들면 다중 스케일 영상 생성 ok**

2단계 : 다중 스케일 영상에 미분 적용

**== 스케일 정보 알아내기**

정규 라플라시안 사용

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/291c52ee-d6e4-4c03-a5cb-515f3021b7f2/Untitled.png)

→ 단점 : 큰 필터로 컨볼루션 수행하여 시간 많이 걸림 

단점 보완, 유사기능 보유 **DOG**  Different of Gaussian 

- 이웃한 영상을 화소별로 빼면 되어 아주 빠르게 계산 가능

3단계 : 극점 검출

**== 특징점의 위치 알아내기**

i 번째 영상에서 X 표시된 화소의 극점 여부 조사

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ea17a567-3880-49a6-829c-12c103d9975a/Untitled.png)

→ 이웃 영상까지 조사해 18개 이웃 화소 조사

++ 기술사 추출 단계

2단계 3단계로 확인한 위치와 스케일 정보만으로 물체 매칭하는데 정보가 턱없이 부족

→ **특징점 주위를 살펴** 풍부한 정보를 가진 기술자 추출 단계

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5e96b4c3-6ce8-4f43-b185-25a915e96ba0/Untitled.png)

위 조건 만족 필요

<알고리즘>

1. 가장 가까운 가우시안 영상 결정하고 거기서 기술자 추출
    
    → 스케일 불변성 달성
    
2. 기준 방향 설정, 기준 방향 중심으로 특징 추출
    
    → 회전 불변성 달성
    
3. 기술자 x를 단위 벡터로 변경 & 단위벡터에 0.2보다 큰 요소 있으면 0.2로 변경→ 단위 벡터
    
    → 조명 불변성 달성
    

```python
import cv2 as cv

img=cv.imread('mot_color70.jpg') # 영상 읽기
gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)

sift=cv.SIFT_create() # SIFT 특징점 추출하는데 쓸 객체 생성
kp,des=sift.detectAndCompute(gray,None) # 특징점, 기술자 탐색

# 특징점 영상에 표시
gray=cv.drawKeypoints(gray,kp,None,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

cv.imshow('sift', gray)

k=cv.waitKey()
cv.destroyAllWindows()
```

```python
cv.SIFT_create(nfeatures=0,nOctaveLayers=3,contrastThreshold=0.04,edgeThreshold=10, sigma=1.6) 
```

- 검출할 특징점 개수 지정
    - 0 : 검출한 특징점 모두 반환
    - 개수 : 신뢰도가 높은 순서로 지정한 만큼만 반환
- 옥타브 개수 지정
- 테일러 확장으로 미세조정할때 쓰는 매개변수
    - 값이 클수록 적은 수의 특징점 검출
- 에지에서 검출된 특징점을 걸러내는 데 쓰는 매개변수로 값이 클수록 덜 걸러내 더 많은 특징점 발생
- 옥타브 0의 입력 영상에 적용할 가우시안 표준편차

## 5.5 매칭

특징점이 많고 잡음이 섞인 기술자가 꽤 있다는 것을 고려해 **가장 유사한 특징점 찾아 쌍 맺기**

⇒ 같은 물체의 같은 곳에서 해당하는 쌍을 찾는 문제

- 첫번째 영상에서 추출한 기술자 집합 A = {a1, a2, a3, … , am}
    
    → 물체의 모델 영상(신뢰도가 높은 적은수의 기술자 검출)
    
- 두번째 영상에서 추출한 기술자 집합 B = {b1, b2, b3, … , bn}
    
    → 물체와 배경이 섞인 장면 영상 (기술자는 많고 잡음 심함 ㅠ)
    

mn 개의 쌍 각각에 대해 거리 계산, 거리가 임곗값보다 작은 쌍을 모두 취함

STEP1 거리 계산

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c6180e81-5cb2-41aa-b4c8-7f1989008f58/Untitled.png)

→ 유클리디안 거리 사용해 두 기술자의 거리 계산

STEP2 매칭 전략

1. 고정 임곗값 방법 
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a1d9203d-bd0b-4a9a-a8e4-73eb9514e12f/Untitled.png)
    
    - 두 기술자 거리가 임곗값보다 작으면 매칭 !!
    - 임곗값 T 정하는 일이 중요
    
2. 최근접 이웃
    
    a_i는 B에서 거리가 가장 작은 b_j를 찾고 거리가 임계값보다 작으면 매칭 쌍으로 취함 
    

1. 최근접 이웃 거리 비율
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b5a0c40d-efca-416c-9a75-fbe6408abde0/Untitled.png)
    
    가장 가까운 bj 와 두번째 가까운 bk 찾아 비율로 확인
    

+++ 임계값 T를 작게하면 거짓긍정률은 작아지고, 크게 하면 거짓 긍정률이 커짐

STEP3 매칭 성능 측정 - 정확도 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a12891e7-a208-4cdd-9cf3-0fea16982fab/Untitled.png)

색깔로 정답 표시

- 동일 색 : 진짜 매칭 쌍
- 다른 색 : 가짜 매칭 쌍

1. 긍정 예측-찐긍정
2. 거짓 예측-찐긍정
3. 긍정 예측 -찐부정
4. 부정 예측 - 찐부정

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/03801d40-89c3-4b7f-ba32-eeb8462b5878/Untitled.png)

a,b,c,d의 경우의 빈도를 세어 혼동행렬 생성 → 성능 지표 계산 가능

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7d041cf4-8211-4cc2-8592-8bf4f5827234/Untitled.png)

정밀도 : 매칭 알고리즘이 긍정, 즉 매칭 쌍으로 예측한 개수 중 진짜 쌍인 비율 

재현율 : 진짜 쌍 중에 알고리즘이 찾아낸 쌍의 비율 

정확률 : 옳게 예측한 비율 

STEP4 빠른 매칭 - 속도

속도 : 실기간 처리가 요구되는 상황에서 강한 조건

→ 대용량 데이터를 빠르게 탐색하는 자료구조 중 이진탐색트리 & 해싱

**kd 트리**

![이진 탐색 트리](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d7efa820-8446-44fa-87ac-fc0964e46cdb/Untitled.png)

이진 탐색 트리

루트 노드(12) 기준

- 좌측 트리  모두 루트보다 작음
- 우측 트리 모두 루트보다 큼

→ 질의어를 탐색하는 일은 루트에서 시작해 루트보다 작으면 왼쪽, 크면 오른쪽으로 이동하는 일을 **재귀적 반복**으로 구성 

이진탐색트리를 특징점 매칭에 적용하면 빠른 속도로 달성 가능 

but 특징점 매칭의 독특한 성질 때문에 그대로 적용 불가 

1. 특징점 매칭에서 여러값으로 구성된 기술자 (특징벡터) 비교
2. 이진 탐색 트리는 정확히 같은 값을 찾는 반면 특징점 매칭에서 최근접 이웃 찾아야 함 

kd 트리 적용 과정

- 기술자 집합
    
    $$
    X = {{x_1, x_2, ... , x_m}}
    $$
    
- i 번째 기술자
    
    $$
    x_i = (x_i1, x_i2 , ...x_id) 
    $$
    
    d개의 축 중 어느 것 사용할지 결정 → 축 각각의 분산 계산, 분산이 가장 큰 축 선택
    
    분산이 클수록 트리 균형에 좋음
    

d=2, m=10 로 축은 각각 두개 축 존재 

1. (3,2,6,4,…,6) (1,3,2,4,..11) ⇒ 두번째 축 분산이 더 커 두번째 축 선택
2. (1,3,2,4,..11) 내림차순 하여 중앙값 도출 시 j=5
    
    **x_5** 5번째 기술자가 분할 기준
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/493cb3e3-67dc-419e-b58f-a3a6fc5f1845/Untitled.png)
    

새로운 특징 벡터가 입력되었다고 가정하고 트리에서 최근접이웃 찾기 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9a218a78-9cbf-4861-8393-f3f2bde9def1/Untitled.png)

**위치 의존 해싱**

해시함수 h가 키값을 키가 저장될 칸의 번호로 매핑

ex. h(134) = 134%13 = 4

해시함수를 한번만 계산하면 키가 들어있는 칸을 찾을 수 있어 빠르게 탐색 가능

비슷한 특징 벡터가 같은 칸에 담길 확률을 최대화해야 함 

+) **FLANN,  FLAISS**

kd 트리의 다양한 변형을 보여주는 함수

```python
import cv2 as cv
import numpy as np
import time

# 물체 모델 영상 정하기
img1=cv.imread('mot_color70.jpg')[190:350,440:560] # 버스를 크롭하여 모델 영상으로 사용
gray1=cv.cvtColor(img1,cv.COLOR_BGR2GRAY)

# 물체 장면 영상 정하기
img2=cv.imread('mot_color83.jpg')			     
gray2=cv.cvtColor(img2,cv.COLOR_BGR2GRAY)

sift=cv.SIFT_create()
kp1,des1=sift.detectAndCompute(gray1,None)
kp2,des2=sift.detectAndCompute(gray2,None)
print('특징점 개수:',len(kp1),len(kp2)) 

start=time.time()
flann_matcher=cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED) # FLANN 라이브러리 사용
knn_match=flann_matcher.knnMatch(des1,des2,2) # 최근접 두개 찾기

# 임계값 이용해 최근접 이웃거리 비율 전략 적용
T=0.7
good_match=[]
for nearest1,nearest2 in knn_match:
    if (nearest1.distance/nearest2.distance)<T:
        good_match.append(nearest1)
print('매칭에 걸린 시간:',time.time()-start) 

# 매칭 결과 보여줄 ㄹ영상 생성
img_match=np.empty((max(img1.shape[0],img2.shape[0]),img1.shape[1]+img2.shape[1],3),dtype=np.uint8)
cv.drawMatches(img1,kp1,img2,kp2,good_match,img_match,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

cv.imshow('Good Matches', img_match)

k=cv.waitKey()
cv.destroyAllWindows()
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3da99814-1ebd-459b-b4fd-16fc2be09255/Untitled.png)

## 5.6 호모그래피 추정

**아웃라이어**(이상치) 걸러내는 과정 필요 

매칭 쌍을 이용해 **물체 위치** 찾는 과정 추가 

호모그래피 : 3차원에서 z 축을 무시해 변환한 3*3행렬로 표현되는 제한된 상황에서 3차원 점이 2차원 평면으로 변환되는 기하 관계인 **투영변환**을 평면 호모그래피

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/08dc55e5-2971-4254-8913-d97ade9612f1/Untitled.png)

평면P : 3차원 물체가 놓인 평면 / 카메라 A 영상 평면 / 카메라 B 영상 평면 

호모그래피 행렬 H : 어떤 평면의 점을 다른 평면의 점으로 투영하는 변환행렬 

앞서 구한 매칭쌍으로 호모그래피 행렬 도출 

ex. 점 a → 점 b 위치로 되게 하는 것은 행렬 H

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/54022559-45be-404f-89b2-16b14ca7c358/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/49d09469-8fb8-47b6-8193-6657601094c0/Untitled.png)

- H_PA : 평면 P를 카메라 A의 영상 평면으로 투영하는 호모그래피 행렬
- H_PB : 카메라 B의 영상평면으로 투영
- H_AB : 카메라 A의 평면을 카메라 B의 평면으로 투영

행렬 H구하는 알고리즘 - 최소평균제곱오차, RANSAC

### 최소평균제곱오차

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e4ac61fb-ea46-40e4-bfcf-f92b498e8870/Untitled.png)

**E가 최소인 H 찾기** 

모든 매칭쌍이 같은 자격으로 오류계산에 참여함 

→ 이상치로 인해 호모그래피 정확도 감소할 수 있음 

**이상치 걸러내기** 

모든 쌍의 n개의 오차를 계산한 다음 정렬한 뒤,  가운데 위치한 오차(중앙값)를 E 취함 ↔ 평균 X 

### RANSAC

표본에 섞여 있는 아웃라이어 회피하며 최적해 구하는 기법

매칭 쌍 집합을 입력으로 받아 최적의 호모그래피 행렬 추정 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ce1e13b8-9aa8-4b34-a51d-1d6444e45785/Untitled.png)

1. 후보 호모그래피 저장할 리스트 초기화 
2. 반복 횟수 수천으로 지정
3. 매칭 쌍 집합 X에서 4쌍을 임의로 선택하고 오류 E가 최소가 되도록 H 추정 
4. 선택된 임의의 4쌍으로 inlier 집합 초기화 
5. ~6. 나머지 매칭 쌍 중 H에 동의하는 쌍 찾아 inlier 집합에 추가 

 7. ~ 9. 모든 쌍 가지고 H추정 및 h 추가 

import cv2 as cv
import numpy as np
import time

# 물체 모델 영상 정하기
img1=cv.imread('mot_color70.jpg')[190:350,440:560] # 버스를 크롭하여 모델 영상으로 사용
gray1=cv.cvtColor(img1,cv.COLOR_BGR2GRAY)

# 물체 장면 영상 정하기
img2=cv.imread('mot_color83.jpg')			     
gray2=cv.cvtColor(img2,cv.COLOR_BGR2GRAY)

sift=cv.SIFT_create()
kp1,des1=sift.detectAndCompute(gray1,None)
kp2,des2=sift.detectAndCompute(gray2,None)
print('특징점 개수:',len(kp1),len(kp2)) 

start=time.time()
flann_matcher=cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED) # FLANN 라이브러리 사용
knn_match=flann_matcher.knnMatch(des1,des2,2) # 최근접 두개 찾기

# 임계값 이용해 최근접 이웃거리 비율 전략 적용
T=0.7
good_match=[]
for nearest1,nearest2 in knn_match:
    if (nearest1.distance/nearest2.distance)<T:
        good_match.append(nearest1)

```python
# good_match : 매칭 쌍 중 좋은 것을 골라 저장한 리스트
points1=np.float32([kp1[gm.queryIdx].pt for gm in good_match]) # 첫번째 영상 특징점 좌표 
points2=np.float32([kp2[gm.trainIdx].pt for gm in good_match]) # 두번째 영상 특징점 좌표

H,_=cv.findHomography(points1,points2,cv.RANSAC) # 호모그래피 행렬 추정

h1,w1=img1.shape[0],img1.shape[1] 		# 첫 번째 영상의 크기
h2,w2=img2.shape[0],img2.shape[1] 		# 두 번째 영상의 크기

box1=np.float32([[0,0],[0,h1-1],[w1-1,h1-1],[w1-1,0]]).reshape(4,1,2) # 첫번째 영상을 포함하는 네 구석의 좌표 저장
box2=cv.perspectiveTransform(box1,H) # 첫번째 영상 좌표에 행렬 H 적용, 두번째 영상 투영한 결과 저장

img2=cv.polylines(img2,[np.int32(box2)],True,(0,255,0),8) # box2를 두번째 영상에 그리기

img_match=np.empty((max(h1,h2),w1+w2,3),dtype=np.uint8)
cv.drawMatches(img1,kp1,img2,kp2,good_match,img_match,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
   
cv.imshow('Matches and Homography',img_match)

k=cv.waitKey()
cv.destroyAllWindows()
```
